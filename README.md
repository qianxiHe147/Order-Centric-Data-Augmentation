# Order-Centric-Data-Augmentation

## Overview
This repository contains the official implementation of the paper **"Order Doesn't Matter, But Reasoning Does: Training LLMs with Order-Centric Augmentation"**.[ğŸ“„ Read the full paper](https://arxiv.org/abs/2502.19907)

We propose an **order-centric data augmentation framework** to enhance logical reasoning in large language models (LLMs) by leveraging the **commutativity** of premises and structured reordering of reasoning steps. Our experiments demonstrate that this approach significantly improves LLMs' ability to generalize across different logical reasoning structures.

## Methodology
We introduce two key augmentations:
- **Condition Order Augmentation**: Randomly shuffling independent premises to teach models the logical equivalence of condition order.
- **Answer Order Augmentation**: Using a **directed acyclic graph (DAG)** to identify valid step reorderings while maintaining logical dependencies.

This framework allows LLMs to develop a more **flexible** and **generalized** reasoning process, overcoming biases from fixed sequential patterns.

## Overview

This repository provides a structured pipeline for transforming and processing logical reasoning datasets. The workflow is divided into three major components:

1. **Condition Processing**: Prepares and refines condition-based logical reasoning datasets.
2. **Answer Transformation**: Reorders reasoning steps and restructures responses for diverse logical reasoning.
3. **Testing and Evaluation**: Runs validation tests to ensure data integrity and correctness.

---


## Code Structure

```
order_centric/
â”‚â”€â”€ code/
â”‚   â”œâ”€â”€ condition/    # Prepares input conditions
â”‚   â”‚   â”œâ”€â”€ condition_ran.py
â”‚   â”‚
â”‚   â”œâ”€â”€ answer/       # Processes and organizes step-by-step reasoning answers
â”‚   â”‚   â”œâ”€â”€ extract_answer_steps.py
â”‚   â”‚   â”œâ”€â”€ extract_steps_only.py
â”‚   â”‚   â”œâ”€â”€ format_random_cot.py
â”‚   â”‚   â”œâ”€â”€ generate_step_sequences.py
â”‚   â”‚   â”œâ”€â”€ renumber_steps.py
â”‚   â”‚   â”œâ”€â”€ reorganize_steps.py
â”‚   â”‚
â”‚   â”œâ”€â”€ test/         # Evaluates the model performance
â”‚   â”‚   â”œâ”€â”€ accuracy/       # Accuracy evaluation
â”‚   â”‚   â”‚   â”œâ”€â”€ folio_acc.py
â”‚   â”‚   â”‚   â”œâ”€â”€ logicnli_acc.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ruletaker_acc.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ inference/      # Inference scripts
â”‚   â”‚   â”‚   â”œâ”€â”€ folio_vllm.py
â”‚   â”‚   â”‚   â”œâ”€â”€ logicnli_vllm.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ruletaker_vllm.py
```

---

## Execution 

### 1ï¸âƒ£ Condition Order Augmentation (`condition/`)

- Prepares the dataset by modifying input conditions before answer processing.

### 2ï¸âƒ£ Answer Order Augmentation (`answer/`)

This stage processes answer data, ensuring logical order, formatting, and restructuring.

1. **Extract Steps**:
   - `extract_answer_steps.py`: Extracts step-by-step reasoning paths from the dataset.
   - `extract_steps_only.py`: Isolates only the reasoning steps without additional text.
2. **Generate Logical Sequences**:
   - `generate_step_sequences.py`: Creates different orderings of reasoning steps.
3. **Reorganize and Format**:
   - `reorganize_steps.py`: Reorders steps based on logical dependencies.
   - `renumber_steps.py`: Renumbers steps after reorganization.
   - `format_random_cot.py`: Converts reasoning steps into a structured format for training.

### 3ï¸âƒ£ Testing (`test/`)

#### **Inference :**

- `folio_vllm.py`: Runs inference on the FOLIO dataset.
- `logicnli_vllm.py`: Runs inference on the LogicNLI dataset.
- `ruletaker_vllm.py`: Runs inference on the RuleTaker dataset.


#### **Accuracy Evaluation :**

- `folio_acc.py`: Evaluates accuracy on the FOLIO dataset.
- `logicnli_acc.py`: Evaluates accuracy on LogicNLI dataset.
- `ruletaker_acc.py`: Evaluates accuracy on RuleTaker dataset.

---

## Notes

- Ensure dependencies (such as Python libraries) are installed before running the scripts.

---
